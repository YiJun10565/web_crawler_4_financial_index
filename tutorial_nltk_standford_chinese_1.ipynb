{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "請記得要先到 https://drive.google.com/drive/folders/1vwCTfeUuY5eAbqEdhmsayzEE6Bd7HSE6?usp=sharing \n",
    "下載整個「StanfordNLP」資料夾(名稱要一樣)，並根這份程式放在同一個目錄下，接著anaconda要安裝nltk，就可以開始使用啦!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-segmenter.jar jar file at ./StanfordNLP/jars/stanford-segmenter-3.9.2.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1ed5232ff2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpath_to_sihan_corpora_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpath_to_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data/pku.gz\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpath_to_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data/dict-chris6.ser.gz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"副董事长陈焕光，涉嫌利用假交易、洗钱等方式，在台中掏空诚美材子公司茂丰贸易公司资产约2.6亿\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\stanford_segmenter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_slf4j, java_class, path_to_model, path_to_dict, path_to_sihan_corpora_dict, sihan_post_processing, keep_whitespaces, encoding, options, verbose, java_options)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_to_slf4j\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    851\u001b[0m     return next(\n\u001b[0;32m    852\u001b[0m         find_jar_iter(\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         )\n\u001b[0;32m    855\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             raise LookupError(\n\u001b[1;32m--> 739\u001b[1;33m                 \u001b[1;34m'Could not find %s jar file at %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             )\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-segmenter.jar jar file at ./StanfordNLP/jars/stanford-segmenter-3.9.2.jar"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import StanfordSegmenter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "#要記得將github上的StanfordNLP下載下來\n",
    "#這是比較直覺(攏長)的用法，直接指定路徑，而不設定環境變數\n",
    "segmenter = StanfordSegmenter(\n",
    "    java_class='edu.stanford.nlp.ie.crf.CRFClassifier',\n",
    "    path_to_jar=\"./StanfordNLP/jars/stanford-segmenter-3.9.2.jar\",\n",
    "    path_to_slf4j=\"./StanfordNLP/jars/slf4j-api.jar\",\n",
    "    path_to_sihan_corpora_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data\",\n",
    "    path_to_model=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/pku.gz\",\n",
    "    path_to_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/dict-chris6.ser.gz\"\n",
    ")\n",
    "res = segmenter.segment(u\"副董事长陈焕光，涉嫌利用假交易、洗钱等方式，在台中掏空诚美材子公司茂丰贸易公司资产约2.6亿\")\n",
    "\n",
    "print(type(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "副董事长 O\n",
      "陈 PERSON\n",
      "焕光 O\n",
      "， O\n",
      "涉嫌 O\n",
      "利用 O\n",
      "假 O\n",
      "交易 O\n",
      "、 O\n",
      "洗钱 O\n",
      "等 O\n",
      "方式 O\n",
      "， O\n",
      "在 O\n",
      "台中 GPE\n",
      "掏空 O\n",
      "诚 O\n",
      "美 O\n",
      "材 O\n",
      "子公司 O\n",
      "茂 O\n",
      "丰 O\n",
      "贸易 O\n",
      "公司 O\n",
      "资产 O\n",
      "约 O\n",
      "2.6亿 O\n"
     ]
    }
   ],
   "source": [
    "#這是比較直覺(攏長)的用法，直接指定路徑\n",
    "chi_tagger = StanfordNERTagger('./StanfordNLP/models/chinese.misc.distsim.crf.ser.gz',\n",
    "                              './StanfordNLP/jars/stanford-ner.jar')\n",
    "\n",
    "for word, tag in  chi_tagger.tag(res.split()):\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刑事局 NN\n",
      "接获 VV\n",
      "中国 NR\n",
      "大陆 NN\n",
      "公安 NN\n",
      "通报 VV\n",
      "有 VE\n",
      "多起 VV\n",
      "假 JJ\n",
      "检警 NN\n",
      "诈骗 NN\n",
      "案 NN\n",
      "， PU\n",
      "经 P\n",
      "查 VV\n",
      "诈欺 NN\n",
      "集团 NN\n",
      "据点 NN\n",
      "遍布 VV\n",
      "台湾 NR\n",
      "北 NN\n",
      "中 LC\n",
      "南 NN\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "import os\n",
    "#這是比較直覺(攏長)的用法，直接指定路徑\n",
    "chi_tagger = StanfordPOSTagger('./StanfordNLP/models/chinese-distsim.tagger',\n",
    "                               './StanfordNLP/jars/stanford-ner.jar')\n",
    "sent = u'刑事局 接获 中国 大陆 公安 通报 有 多起 假 检警 诈骗 案 ， 经 查 诈欺 集团 据点 遍布 台湾 北 中 南 '\n",
    "for _, word_and_tag in  chi_tagger.tag(sent.split()):\n",
    "    word, tag = word_and_tag.split('#')\n",
    "    print(word , tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-segmenter.jar jar file at ./StanfordNLP/jars/stanford-segmenter-3.9.2.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-953a6520ec99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mpath_to_sihan_corpora_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mpath_to_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data/pku.gz\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mpath_to_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./StanfordNLP/stanford-segmenter-2018-10-16/data/dict-chris6.ser.gz\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegmenter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\stanford_segmenter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_slf4j, java_class, path_to_model, path_to_dict, path_to_sihan_corpora_dict, sihan_post_processing, keep_whitespaces, encoding, options, verbose, java_options)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0msearchpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         )\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_to_slf4j\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    851\u001b[0m     return next(\n\u001b[0;32m    852\u001b[0m         find_jar_iter(\n\u001b[1;32m--> 853\u001b[1;33m             \u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m         )\n\u001b[0;32m    855\u001b[0m     )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             raise LookupError(\n\u001b[1;32m--> 739\u001b[1;33m                 \u001b[1;34m'Could not find %s jar file at %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             )\n\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-segmenter.jar jar file at ./StanfordNLP/jars/stanford-segmenter-3.9.2.jar"
     ]
    }
   ],
   "source": [
    "#這個cell 會先將to_convert的繁體字轉簡體後，再用StanfordSegmenter分詞，最後用StanfordParser將詞性與樹狀結構標註，並畫出來\n",
    "#轉成簡體字結果會比較準確\n",
    "import os\n",
    "from opencc import OpenCC\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import StanfordSegmenter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "#但遇到32行的「StanfordParser」的class，就必須要如 11-14 行一樣設定環境變數，不能再用上述偷懶直接指定路徑的方法\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-11.0.1\"#注意這邊你們電腦要安裝java jdk，並放入你們自己的jdk的安裝路徑\n",
    "\n",
    "os.environ[\"CLASSPATH\"] = \"./StanfordNLP/jars\"\n",
    "os.environ[\"STANFORD_MODELS\"] = \"./StanfordNLP/models\"\n",
    "\n",
    "to_convert=\"刑事局接獲中國大陸公安通報有多起假檢警詐騙案，經查詐欺集團據點遍布台灣北中南\"\n",
    "cc = OpenCC('t2s')  # (Optional )convert from Simplified Chinese to Traditional Chinese\n",
    "content = cc.convert(to_convert)#一樣轉成簡體字，這樣Stanford的系統才會比較準確\n",
    "\n",
    "\n",
    "segmenter = StanfordSegmenter(\n",
    "    java_class='edu.stanford.nlp.ie.crf.CRFClassifier',\n",
    "    path_to_jar=\"./StanfordNLP/jars/stanford-segmenter-3.9.2.jar\",\n",
    "    path_to_slf4j=\"./StanfordNLP/jars/slf4j-api.jar\",\n",
    "    path_to_sihan_corpora_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data\",\n",
    "    path_to_model=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/pku.gz\",\n",
    "    path_to_dict=\"./StanfordNLP/stanford-segmenter-2018-10-16/data/dict-chris6.ser.gz\"\n",
    ")\n",
    "res = segmenter.segment(content)\n",
    "\n",
    "\n",
    "ch_parser = StanfordParser(model_path=u'./StanfordNLP/models/chinesePCFG.ser.gz')\n",
    "sentences  = ch_parser.parse(res.split())\n",
    "\n",
    "for line in sentences:\n",
    "    for sentence in line:\n",
    "        sentence.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.888 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['刑事', '局', '接获', '中国', '大陆', '公安', '通报', '有多起', '假检警', '诈骗案', '，', '经查', '诈欺', '集团', '据点', '遍布', '台湾', '北', '中南']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0e9dd1fc6d2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mch_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'./StanfordNLP/models/chinesePCFG.ser.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0msentences\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mch_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m         )\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mis_regex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             ),\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         )\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    837\u001b[0m             )\n\u001b[0;32m    838\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "#這是斷詞系統用結疤，然後搭配StanfordParser的例子，你們可以與上一個cell做比較\n",
    "import jieba\n",
    "from opencc import OpenCC\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import StanfordSegmenter\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "to_convert=\"刑事局接獲中國大陸公安通報有多起假檢警詐騙案，經查詐欺集團據點遍布台灣北中南\"\n",
    "cc = OpenCC('t2s')  # (Optional )convert from Simplified Chinese to Traditional Chinese\n",
    "content = cc.convert(to_convert)#一樣轉成簡體字，這樣Stanford的系統才會比較準確\n",
    "\n",
    "word_list = list(jieba.cut(content, cut_all=False))\n",
    "print(word_list)\n",
    "\n",
    "ch_parser = StanfordParser(model_path=u'./StanfordNLP/models/chinesePCFG.ser.gz')\n",
    "sentences  = ch_parser.parse(word_list)\n",
    "\n",
    "for line in sentences:\n",
    "    for sentence in line:\n",
    "        sentence.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
